#!/usr/bin/env python3
"""
Comprehensive Ramanarunachalam Music Repository Processor
========================================================

This script processes the massive Ramanarunachalam repository to extract:
- Raga metadata and relationships
- Artist information and performance data
- Composer details and compositions
- Cross-tradition mappings (Carnatic vs Hindustani)
- Multi-language support analysis
- Statistical insights and patterns

Features:
- GPU-accelerated processing for MacBook
- Multi-threaded JSON parsing
- Cross-tradition raga mapping
- W&B experiment tracking
- Comprehensive data validation
- Export to multiple formats (JSON, CSV, Neo4j)
"""

import json
import time
import concurrent.futures
import wandb
import torch
import platform
from datetime import datetime
from pathlib import Path
import sys
import logging
from typing import Dict, Any, List, Optional, Set, Tuple
import pandas as pd
from collections import defaultdict, Counter
import re

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from tools.utils.macbook_gpu_accelerator import MacBookGPUAccelerator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('comprehensive_ramanarunachalam_processor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ComprehensiveRamanarunachalamProcessor:
    """
    A comprehensive processor for the Ramanarunachalam Music Repository.
    Extracts raga metadata, artist information, composer data, and builds
    cross-tradition mappings with GPU acceleration.
    """
    
    def __init__(self, repo_path: Path, max_workers: int = 8):
        self.repo_path = repo_path
        self.output_path = project_root / "data" / "comprehensive_ramanarunachalam_analysis"
        self.output_path.mkdir(parents=True, exist_ok=True)
        self.max_workers = max_workers
        
        # Initialize GPU accelerator
        self.gpu_accelerator = MacBookGPUAccelerator()
        if self.gpu_accelerator.gpu_available:
            logger.info(f"ğŸ MacBook GPU ({self.gpu_accelerator.get_device()}) acceleration enabled")
        else:
            logger.warning("âš ï¸ GPU not available, using CPU for processing.")
            
        # Initialize W&B
        try:
            wandb.init(
                project="ragasense-comprehensive-analysis", 
                name=f"ramanarunachalam-{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            self.wandb_available = True
            logger.info("âœ… W&B tracking enabled")
        except Exception as e:
            self.wandb_available = False
            logger.warning(f"âš ï¸ W&B not available: {e}")
        
        # Data storage
        self.ragas = {}
        self.artists = {}
        self.composers = {}
        self.songs = {}
        self.cross_tradition_mappings = {}
        self.language_stats = defaultdict(int)
        self.tradition_stats = defaultdict(int)
        
        # Processing statistics
        self.stats = {
            'total_files_processed': 0,
            'ragas_found': 0,
            'artists_found': 0,
            'composers_found': 0,
            'songs_found': 0,
            'cross_tradition_mappings': 0,
            'processing_time': 0,
            'gpu_acceleration_used': self.gpu_accelerator.gpu_available
        }

    def _process_json_file(self, file_path: Path) -> Dict[str, Any]:
        """Process a single JSON file and extract relevant metadata."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Determine file type and extract relevant information
            relative_path = str(file_path.relative_to(self.repo_path))
            file_type = self._classify_file_type(relative_path)
            
            result = {
                'file_path': relative_path,
                'file_type': file_type,
                'data': data,
                'processed_at': datetime.now().isoformat()
            }
            
            # GPU-accelerated processing for numerical data
            if self.gpu_accelerator.gpu_available:
                result['gpu_processed'] = self._gpu_process_data(data)
            
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON decoding error in {file_path}: {e}")
            return {'file_path': str(file_path.relative_to(self.repo_path)), 'error': f"JSON error: {e}"}
        except Exception as e:
            logger.error(f"âŒ Error processing file {file_path}: {e}")
            return {'file_path': str(file_path.relative_to(self.repo_path)), 'error': str(e)}

    def _classify_file_type(self, file_path: str) -> str:
        """Classify the type of file based on its path."""
        if '/raga/' in file_path:
            return 'raga'
        elif '/artist/' in file_path:
            return 'artist'
        elif '/composer/' in file_path:
            return 'composer'
        elif 'raga.json' in file_path:
            return 'raga_index'
        elif 'artist.json' in file_path:
            return 'artist_index'
        elif 'composer.json' in file_path:
            return 'composer_index'
        elif 'concert.json' in file_path:
            return 'concert'
        else:
            return 'other'

    def _gpu_process_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Process data using GPU acceleration for numerical operations."""
        try:
            gpu_result = {}
            
            # Extract numerical data for GPU processing
            if 'stats' in data and isinstance(data['stats'], list):
                # Process statistics arrays
                stats_tensor = torch.tensor([len(data['stats'])], dtype=torch.float32)
                gpu_stats = self.gpu_accelerator.to_device(stats_tensor)
                gpu_result['stats_count'] = gpu_stats.item()
            
            if 'songs' in data and isinstance(data['songs'], list):
                # Process song counts
                songs_tensor = torch.tensor([len(data['songs'])], dtype=torch.float32)
                gpu_songs = self.gpu_accelerator.to_device(songs_tensor)
                gpu_result['songs_count'] = gpu_songs.item()
            
            return gpu_result
            
        except Exception as e:
            logger.warning(f"âš ï¸ GPU processing failed: {e}")
            return {}

    def _extract_raga_metadata(self, file_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract comprehensive raga metadata from processed files."""
        if file_data['file_type'] != 'raga':
            return {}
        
        data = file_data['data']
        raga_info = {
            'name': data.get('title', {}).get('H', 'Unknown'),
            'sanskrit_name': data.get('title', {}).get('V', ''),
            'stats': data.get('stats', []),
            'songs': data.get('songs', []),
            'languages': data.get('languages', {}),
            'file_path': file_data['file_path']
        }
        
        # Extract song count and duration
        if raga_info['songs']:
            raga_info['song_count'] = len(raga_info['songs'])
            # Extract duration from first song if available
            first_song = raga_info['songs'][0]
            if 'D' in first_song:
                raga_info['sample_duration'] = first_song['D']
        
        return raga_info

    def _extract_artist_metadata(self, file_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract comprehensive artist metadata from processed files."""
        if file_data['file_type'] != 'artist':
            return {}
        
        data = file_data['data']
        artist_info = {
            'name': data.get('title', {}).get('H', 'Unknown'),
            'stats': data.get('stats', []),
            'songs': data.get('songs', []),
            'languages': data.get('languages', {}),
            'file_path': file_data['file_path']
        }
        
        # Extract performance statistics
        if artist_info['songs']:
            artist_info['song_count'] = len(artist_info['songs'])
            # Calculate total views if available
            total_views = 0
            for song in artist_info['songs']:
                if 'V' in song:
                    try:
                        views = int(song['V'].replace(',', '').replace('K', '000').replace('M', '000000'))
                        total_views += views
                    except:
                        pass
            artist_info['total_views'] = total_views
        
        return artist_info

    def _extract_composer_metadata(self, file_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract comprehensive composer metadata from processed files."""
        if file_data['file_type'] != 'composer':
            return {}
        
        data = file_data['data']
        composer_info = {
            'name': data.get('title', {}).get('H', 'Unknown'),
            'stats': data.get('stats', []),
            'songs': data.get('songs', []),
            'languages': data.get('languages', {}),
            'file_path': file_data['file_path']
        }
        
        # Extract composition statistics
        if composer_info['songs']:
            composer_info['composition_count'] = len(composer_info['songs'])
        
        return composer_info

    def _build_cross_tradition_mappings(self) -> Dict[str, Any]:
        """Build cross-tradition mappings between Carnatic and Hindustani ragas."""
        logger.info("ğŸ”— Building cross-tradition raga mappings...")
        
        # Common raga names that appear in both traditions
        carnatic_ragas = set()
        hindustani_ragas = set()
        
        # Extract raga names from both traditions
        for raga_name, raga_data in self.ragas.items():
            if 'Carnatic' in raga_data.get('file_path', ''):
                carnatic_ragas.add(raga_name.lower())
            elif 'Hindustani' in raga_data.get('file_path', ''):
                hindustani_ragas.add(raga_name.lower())
        
        # Find common ragas
        common_ragas = carnatic_ragas.intersection(hindustani_ragas)
        
        # Build similarity mappings
        mappings = {
            'identical': list(common_ragas),
            'carnatic_unique': list(carnatic_ragas - hindustani_ragas),
            'hindustani_unique': list(hindustani_ragas - carnatic_ragas),
            'total_carnatic': len(carnatic_ragas),
            'total_hindustani': len(hindustani_ragas),
            'common_count': len(common_ragas)
        }
        
        self.stats['cross_tradition_mappings'] = len(common_ragas)
        return mappings

    def _analyze_language_distribution(self) -> Dict[str, Any]:
        """Analyze the distribution of languages across the dataset."""
        logger.info("ğŸŒ Analyzing language distribution...")
        
        language_stats = defaultdict(int)
        total_songs = 0
        
        for raga_data in self.ragas.values():
            languages = raga_data.get('languages', {})
            for lang, count in languages.items():
                if isinstance(count, (int, str)) and str(count).isdigit():
                    language_stats[lang] += int(count)
                    total_songs += int(count)
        
        # Calculate percentages
        language_percentages = {}
        for lang, count in language_stats.items():
            if total_songs > 0:
                language_percentages[lang] = (count / total_songs) * 100
        
        return {
            'language_counts': dict(language_stats),
            'language_percentages': language_percentages,
            'total_songs': total_songs
        }

    def process_repository(self):
        """Process the entire Ramanarunachalam repository."""
        logger.info("ğŸµ COMPREHENSIVE RAMANARUNACHALAM PROCESSING")
        logger.info("=" * 60)
        logger.info(f"ğŸ§µ Max workers: {self.max_workers}")
        logger.info(f"ğŸ GPU acceleration: {'âœ…' if self.gpu_accelerator.gpu_available else 'âŒ'}")
        
        # Find all JSON files
        json_files = list(self.repo_path.rglob('*.json'))
        logger.info(f"ğŸ“Š Found {len(json_files)} JSON files")
        
        start_time = time.time()
        processed_results = []
        
        # Process files in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_file = {executor.submit(self._process_json_file, file_path): file_path 
                            for file_path in json_files}
            
            for i, future in enumerate(concurrent.futures.as_completed(future_to_file)):
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    processed_results.append(result)
                    
                    # Extract metadata based on file type
                    if result.get('file_type') == 'raga':
                        raga_metadata = self._extract_raga_metadata(result)
                        if raga_metadata:
                            self.ragas[raga_metadata['name']] = raga_metadata
                            self.stats['ragas_found'] += 1
                    
                    elif result.get('file_type') == 'artist':
                        artist_metadata = self._extract_artist_metadata(result)
                        if artist_metadata:
                            self.artists[artist_metadata['name']] = artist_metadata
                            self.stats['artists_found'] += 1
                    
                    elif result.get('file_type') == 'composer':
                        composer_metadata = self._extract_composer_metadata(result)
                        if composer_metadata:
                            self.composers[composer_metadata['name']] = composer_metadata
                            self.stats['composers_found'] += 1
                    
                except Exception as exc:
                    logger.error(f"âŒ {file_path} generated an exception: {exc}")
                
                # Progress reporting
                if (i + 1) % 100 == 0 or (i + 1) == len(json_files):
                    progress = ((i + 1) / len(json_files)) * 100
                    logger.info(f"ğŸ“Š Progress: {progress:.1f}% ({i+1}/{len(json_files)})")
                    if self.wandb_available:
                        wandb.log({
                            "processing_progress": progress,
                            "files_processed": i + 1,
                            "ragas_found": self.stats['ragas_found'],
                            "artists_found": self.stats['artists_found'],
                            "composers_found": self.stats['composers_found']
                        })
        
        end_time = time.time()
        self.stats['processing_time'] = end_time - start_time
        self.stats['total_files_processed'] = len(processed_results)
        
        # Build cross-tradition mappings
        self.cross_tradition_mappings = self._build_cross_tradition_mappings()
        
        # Analyze language distribution
        language_analysis = self._analyze_language_distribution()
        
        # Generate comprehensive report
        self._generate_comprehensive_report(processed_results, language_analysis)
        
        logger.info("\nğŸ‰ COMPREHENSIVE PROCESSING COMPLETED!")
        logger.info(f"â±ï¸ Processing time: {self.stats['processing_time']:.1f} seconds")
        logger.info(f"ğŸ“Š Files processed: {self.stats['total_files_processed']}")
        logger.info(f"ğŸµ Ragas found: {self.stats['ragas_found']}")
        logger.info(f"ğŸ­ Artists found: {self.stats['artists_found']}")
        logger.info(f"ğŸ¼ Composers found: {self.stats['composers_found']}")
        logger.info(f"ğŸ”— Cross-tradition mappings: {self.stats['cross_tradition_mappings']}")
        
        return self.stats

    def _generate_comprehensive_report(self, processed_results: List[Dict], language_analysis: Dict):
        """Generate a comprehensive analysis report."""
        logger.info("ğŸ“‹ Generating comprehensive analysis report...")
        
        # Create analysis summary
        analysis_summary = {
            "timestamp": datetime.now().isoformat(),
            "processing_stats": self.stats,
            "cross_tradition_mappings": self.cross_tradition_mappings,
            "language_analysis": language_analysis,
            "sample_ragas": dict(list(self.ragas.items())[:10]),
            "sample_artists": dict(list(self.artists.items())[:10]),
            "sample_composers": dict(list(self.composers.items())[:10])
        }
        
        # Save comprehensive analysis
        analysis_file = self.output_path / "comprehensive_analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_summary, f, indent=2, ensure_ascii=False)
        
        # Save individual datasets
        self._save_ragas_dataset()
        self._save_artists_dataset()
        self._save_composers_dataset()
        self._save_cross_tradition_mappings()
        
        # Generate CSV exports
        self._export_to_csv()
        
        # Log to W&B
        if self.wandb_available:
            wandb.log({
                "total_processing_time_seconds": self.stats['processing_time'],
                "total_files_processed": self.stats['total_files_processed'],
                "ragas_found": self.stats['ragas_found'],
                "artists_found": self.stats['artists_found'],
                "composers_found": self.stats['composers_found'],
                "cross_tradition_mappings": self.stats['cross_tradition_mappings'],
                "language_distribution": language_analysis['language_counts']
            })
            wandb.finish()
        
        logger.info(f"ğŸ“‹ Comprehensive analysis saved: {analysis_file}")

    def _save_ragas_dataset(self):
        """Save ragas dataset to JSON file."""
        ragas_file = self.output_path / "ragas_dataset.json"
        with open(ragas_file, 'w', encoding='utf-8') as f:
            json.dump(self.ragas, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸµ Ragas dataset saved: {ragas_file}")

    def _save_artists_dataset(self):
        """Save artists dataset to JSON file."""
        artists_file = self.output_path / "artists_dataset.json"
        with open(artists_file, 'w', encoding='utf-8') as f:
            json.dump(self.artists, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ­ Artists dataset saved: {artists_file}")

    def _save_composers_dataset(self):
        """Save composers dataset to JSON file."""
        composers_file = self.output_path / "composers_dataset.json"
        with open(composers_file, 'w', encoding='utf-8') as f:
            json.dump(self.composers, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ¼ Composers dataset saved: {composers_file}")

    def _save_cross_tradition_mappings(self):
        """Save cross-tradition mappings to JSON file."""
        mappings_file = self.output_path / "cross_tradition_mappings.json"
        with open(mappings_file, 'w', encoding='utf-8') as f:
            json.dump(self.cross_tradition_mappings, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ”— Cross-tradition mappings saved: {mappings_file}")

    def _export_to_csv(self):
        """Export datasets to CSV format for analysis."""
        logger.info("ğŸ“Š Exporting datasets to CSV format...")
        
        # Export ragas to CSV
        if self.ragas:
            ragas_df = pd.DataFrame([
                {
                    'name': raga_data['name'],
                    'sanskrit_name': raga_data.get('sanskrit_name', ''),
                    'song_count': raga_data.get('song_count', 0),
                    'sample_duration': raga_data.get('sample_duration', ''),
                    'tradition': 'Carnatic' if 'Carnatic' in raga_data.get('file_path', '') else 'Hindustani',
                    'file_path': raga_data.get('file_path', '')
                }
                for raga_data in self.ragas.values()
            ])
            ragas_df.to_csv(self.output_path / "ragas_dataset.csv", index=False)
        
        # Export artists to CSV
        if self.artists:
            artists_df = pd.DataFrame([
                {
                    'name': artist_data['name'],
                    'song_count': artist_data.get('song_count', 0),
                    'total_views': artist_data.get('total_views', 0),
                    'tradition': 'Carnatic' if 'Carnatic' in artist_data.get('file_path', '') else 'Hindustani',
                    'file_path': artist_data.get('file_path', '')
                }
                for artist_data in self.artists.values()
            ])
            artists_df.to_csv(self.output_path / "artists_dataset.csv", index=False)
        
        logger.info("ğŸ“Š CSV exports completed")

def main():
    """Main execution function."""
    repo_path = project_root / "downloads" / "Ramanarunachalam_Music_Repository"
    
    if not repo_path.exists():
        logger.error(f"âŒ Repository not found: {repo_path}")
        return
    
    processor = ComprehensiveRamanarunachalamProcessor(repo_path, max_workers=8)
    stats = processor.process_repository()
    
    logger.info("\nğŸ¯ PROCESSING SUMMARY:")
    logger.info(f"   ğŸ“ Files processed: {stats['total_files_processed']}")
    logger.info(f"   ğŸµ Ragas extracted: {stats['ragas_found']}")
    logger.info(f"   ğŸ­ Artists extracted: {stats['artists_found']}")
    logger.info(f"   ğŸ¼ Composers extracted: {stats['composers_found']}")
    logger.info(f"   ğŸ”— Cross-tradition mappings: {stats['cross_tradition_mappings']}")
    logger.info(f"   â±ï¸ Processing time: {stats['processing_time']:.1f} seconds")
    logger.info(f"   ğŸ GPU acceleration: {'âœ…' if stats['gpu_acceleration_used'] else 'âŒ'}")

if __name__ == "__main__":
    main()
